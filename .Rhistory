step$formula
mod_best <- glm(data=df, family="binomial", formula = step$formula)
compare_performance(m6,m7,mod_best) %>% plot
View(df)
p <- penguins
View(p)
glm(data=p,formula=bill_length_mm ~ species*island*bill_depth_mm*flipper_length_mm*body_mass_g*sex*year)
m8 <- glm(data=p,formula=bill_length_mm ~ species*island*bill_depth_mm*flipper_length_mm*body_mass_g*sex*year)
stepAIC(m8)
step1 <- stepAIC(m8)
step1$formula
# magic shortcut
m8 <- glm(data=p,formula=bill_length_mm ~ .^2)
step1 <- stepAIC(m8)
step1$formula
best_mod1 <-  glm(data=penguins,formula=step1$formula)
library(tidyverse)
library(palmerpenguins)
library(easystats)
penguins %>%
ggplot(aes(y=bill_depth_mm,x=bill_length_mm)) +
geom_point() +
geom_smooth(method='lm')
head(penguins)
# make linear regression of this plot
m1 <-
glm(data = penguins,
formula =  bill_depth_mm ~ bill_length_mm)
# according to this model, the average bill depth is 20.88547
# -0.08502 is the slope (m)
# we always visualize first and then we model
# analysis of variance
aov(data = penguins,
formula =  bill_depth_mm ~ bill_length_mm) %>%
summary()
# tells you degrees of freedom, sum of squares, mean fo squares
# this is an anova table. Pr(>F) is the p-value, this only tells you
# if your data is significant, if you chose a good variable comparison.
penguins %>%
ggplot(aes(y=bill_depth_mm,x=bill_length_mm,color=species)) +
geom_point() +
geom_smooth(method='lm',aes(linetype=sex)) +
facet_wrap(~island)
# this is showing simpson's paradox. Now that they're separated, the
# opposite trend shows now.
# just adding, this takes into account the different species.
m2 <-
glm(data = penguins,
formula =  bill_depth_mm ~ bill_length_mm + species)
# the table is more confusing now. The intercept is 10.6
# this picks the default categorical variables based on the alphabet,
# adele became the intercept species. The other estimates tell you what
# to subtract from the default
# here, slope is the same for all the species
# introducing the interaction term between bill length and species - *
m3 <-
glm(data = penguins,
formula =  bill_depth_mm ~ bill_length_mm * species)
# here the slope can change based on the species
# "tell me the effect of species AND the slope"
# aic is telling you which is capturing reality the best.
m1$aic
m2$aic
m2$aic
m4 <-
glm(data=penguins,
formula=bill_depth_mm ~ bill_length_mm * species + sex + island)
# what would a plot of this look like?
# same as before but facet_wrap on sex. or geom_smooth could have a
# aes(linetype=sex)
# from easystats
compare_performance(m1,m2,m3,m4) %>% plot
# R2 tells you how much variance there are to the lines. Tells you the percent
# improvement with your models
# piping to plot shows you the area to compare which is better, larger area
# does better. These show that model 2 is our best model. Our goal with a
# model is simplicity ( and generalized). It won't be perfect, but it needs
# to be helpful.
formula(m4)
x <-
data.frame(bill_length_mm = c(5000,100),
species = c("Chinstrap","Chinstrap"),
sex = c("male","male"),
island = c("Dream","Dream"))
# we're gonna go with model 4, lets do some predicting
predict(m4,x) # it needs all the variables exactly to do a prediction
mpg %>%
ggplot(aes(x=(displ),y=hwy)) +
geom_point() +
geom_smooth(method='lm',formula = y ~ log(x))
# here the model lied to us, if you know more about cars, you can see the
# error in model. The bigger the v8 engine, you do not get better gas mileage
y <- data.frame(displ=500)
m5 <- glm(data=mpg, formula=hwy~displ)
predict(m5,y) # thermodynamics says this is not possible
# it is important to not predict outside of the data you've seen.
a <- data.frame(displ=4.5)
predict(m5,a)
# transform the data
m6 <- glm(data=mpg, formula=hwy~log(displ))
predict(m5,a)
# new built in
Titanic %>%  as.data.frame()
# so far we've been predicting continuous variable, what about categorical
#### logistic regression -> outcome is T/F ####
df <- read_csv("./Data/GradSchool_Admissions.csv")
df <-
df %>%
mutate(admit = as.logical(admit))
# logistic -> family = 'binomial'
m6 <- glm(data = df,
formula = admit~gre+gpa+rank,
family = "binomial") # the predictors are gre, gpa, and rank (separate)
m6 # these are log odds, we want percentages
# interaction is shown with the *
m7 <- glm(data = df,
formula = admit~gre*gpa*rank,
family = "binomial")
compare_performance(m6,m7) %>% plot # comparing the models
# magic trick to get the mathematically best model - might not fit reality
# come up with the best most complicated model.
library(MASS)
m7 <- glm(data = df,
formula = admit~gre*gpa*rank,
family = "binomial")
step <- stepAIC(m7)
# AIC will find the simplelist best model
step$formula # this spits out the best simplified model
mod_best <- glm(data=df, family="binomial", formula = step$formula)
compare_performance(m6,m7,mod_best) %>% plot
# magic shortcut
full_mod <- glm(data=p,formula=bill_length_mm ~ .^2)
compare_performance(full_mod,best_mod1) %>% plot
.2*344
penguins
caret::
rbinom(10,1,.5)
rbinom(nrow(penguins),1,.8)
penguins <-
penguins %>%
mutate(newcol=rbinom(nrow(penguins),1,.8))
View(penguins)
train <- penguins %>% filter(newcol == 1)
test <- penguins %>% filter(newcol == 0)
.2*344 # let's train on 80%, test on 20%
mod_best <- glm(data=train,formula=step1$formula)
# test model on test set
predictions <-
add_predictions(test,mod_best)
# test model on test set
predictions <-
add_predictions(test,mod_best)
library(modelr)
# test model on test set
predictions <-
add_predictions(test,mod_best)
View(predictions)
predictions %>%
mutate(resid = abs(pred - bill_length_mm)) # what is leftover
mean(predictions$resid)
mean(predictions$resid,na.rm=TRUE)
# calculate the absolute difference between actual and predicted bill length
predictions %>%
mutate(resid = abs(pred - bill_length_mm)) # what is leftover
# calculate the absolute difference between actual and predicted bill length
predictions <-
predictions %>%
mutate(resid = abs(pred - bill_length_mm)) # what is leftover
mean(predictions$resid,na.rm=TRUE)
mean_error <- mean(predictions$resid,na.rm=TRUE)
errors <- c()
for(i in 1:100){
penguins <-
penguins %>%
mutate(newcol=rbinom(nrow(penguins),1,.8))
# set 1s to train and 0s to test
train <- penguins %>% filter(newcol == 1)
test <- penguins %>% filter(newcol == 0)
# train model on train
mod_best <- glm(data=train,formula=step1$formula)
# test model on test set
predictions <-
add_predictions(test,mod_best)
# calculate the absolute difference between actual and predicted bill length
predictions <-
predictions %>%
mutate(resid = abs(pred - bill_length_mm)) # what is leftover
mean_error <- mean(predictions$resid,na.rm=TRUE)
errors[1] <- mean_error
}
errors()
errors
errors <- c()
for(i in 1:100){
penguins <-
penguins %>%
mutate(newcol=rbinom(nrow(penguins),1,.8))
# set 1s to train and 0s to test
train <- penguins %>% filter(newcol == 1)
test <- penguins %>% filter(newcol == 0)
# train model on train
mod_best <- glm(data=train,formula=step1$formula)
# test model on test set
predictions <-
add_predictions(test,mod_best)
# calculate the absolute difference between actual and predicted bill length
predictions <-
predictions %>%
mutate(resid = abs(pred - bill_length_mm)) # what is leftover
mean_error <- mean(predictions$resid,na.rm=TRUE)
errors[i] <- mean_error
}
errors
data.frame(errors) %>%
ggplot(aes(x=errors)) +
geom_density()
install.packages("ranger")
library(ranger)
?ranger()
ranger(Species ~ ., data = iris) # prediction species on everything else
r_mod <- ranger(Species ~ ., data = iris) # prediction species on everything else
# classification is referring to Species names
# based on the data I give, what species do I have?
# downfall - there is no formula out at the end, we don't know what tree was used
# all this can do is make predictions, it won't tell you HOW it made the predictions
pred <- predict(r_mod,iris)
pred$predictions
data.frame(iris$Species,pred$predictions)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
read_csv("./Data/Soil_Predators.csv")
read_csv("../Data/Soil_Predators.csv")
path <- read_csv("../Data/Soil_Predators.csv")
path <- "../Data/Soil_Predators.csv"
df <- read_csv(path)
x <- readLines(path,n=2)
x
badcolnames <- readLines(path,n=1)
badcolnames %>% str_replace_all(",_","_") %>% str_split(",")
badcolnames %>% str_replace_all(",_","_") %>% str_split(",") %>% unlist()
df <- read_csv(path,skip=1,col_names=FALSE)
View(df)
badcolnames <- badcolnames %>% str_replace_all(",_","_") %>% str_split(",") %>% unlist()
df <- df %>% select(-c(X25,X26))
df <- df %>% select(-c(X25,X26))
names(df) <- badcolnames
skimr::skim(df)
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
df$Predator_species %>% unique
df$prey_richness <-
df %>% select(starts_with("Consumption")) %>% rowSums(na.rm=TRUE)
df %>%
ggplot(aes(x=Predator_development_stage,y=prey_richness,color=Predator_sex))+
geom_boxplot()
df %>%
ggplot(aes(x=Predator_development_stage,y=prey_richness,color=Predator_sex))+
geom_boxplot() %>%
theme_bw()
df %>%
ggplot(aes(x=Predator_development_stage,y=prey_richness,color=Predator_sex))+
geom_boxplot() +
theme_bw()
p <- df %>%
ggplot(aes(x=Predator_development_stage,y=prey_richness,color=Predator_sex))+
geom_boxplot() +
theme_bw()
plotly::ggplotly(p)
install.packages("plotly")
plotly::ggplotly(p)
plotly::ggplotly(p)
knitr::opts_chunk$set(echo = TRUE)
Sys.Date()
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(tidyverse)
library(easystats)
mpg
mpg
mpg %>%
ggplot(aes(x=displ,y=cty)) +
geom_point()
mpg %>%
ggplot(aes(x=displ,y=cty)) +
geom_point() +
geom_smooth()
mpg %>%
ggplot(aes(x=displ,y=cty)) +
geom_point() +
geom_smooth(method="lm")
glm(data=mpg, formula= cty~displ)
glm(data=mpg,formula= cty~ .^2)
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(tidyverse)
library(easystats)
library(modelr)
m3 <-  glm(data=penguins,formula=step1$formula)
m3 <-  glm(data=mpg,formula=step1$formula)
step1 <- stepAIC(m8)
step1 <- stepAIC(m2)
library(MASS)
m1 <- glm(data=mpg, formula= cty~displ)
m2 <- glm(data=mpg,formula= cty~ .^2)
m2 <- glm(data=mpg,formula= cty~ .^2)
step1 <- stepAIC(m2)
mpg
m2 <- glm(data=mpg,formula= cty~year*cyl*trans*drv)
step1 <- stepAIC(m2)
step1$formula
m3 <-  glm(data=mpg,formula=step1$formula)
compare_performance(m1,m2,m3) %>% plot
df <- mpg %>%
add_predictions(m3)
df <- mpg %>%
add_predictions(m3)
View(df)
df <- mpg %>%
add_predictions(m3) %>%
ggplot(aes(x=displ,y=cty)) +
geom_point() +
geom_point(aes(y=pred))
df %>%
add_predictions(m3) %>%
ggplot(aes(x=displ,y=cty)) +
geom_point() +
geom_point(aes(y=pred))
df <- mpg
df %>%
add_predictions(m3) %>%
ggplot(aes(x=displ,y=cty)) +
geom_point() +
geom_point(aes(y=pred))
df %>%
add_predictions(m3) %>%
ggplot(aes(x=displ,y=cty)) +
geom_point() +
geom_point(aes(y=pred), col="blue")
df %>%
add_predictions(m3) %>%
ggplot(aes(x=displ,y=cty)) +
geom_point() +
geom_point(aes(y=pred), col="red")
df %>%
add_predictions(m3) %>%
ggplot(aes(x=displ,y=cty)) +
geom_point() +
geom_point(aes(y=pred), col="red") +
geom_smooth(aes(y=pred))
df %>%
add_predictions(m3) %>%
ggplot(aes(x=displ,y=cty)) +
geom_point() +
geom_point(aes(y=pred), col="red") +
geom_smooth(aes(y=pred), method = 'lm')
df
hyp_dat <- as.data.frame(year=c(2008,2003,1999),
cyl=c(4.0,6.0,8.0),
trans=c("auto(l5)","(manual(m6)","auto(av)"),
drv=c("f","4","f"))
df %>%
add_predictions(m3) %>%
ggplot(aes(x=displ,y=cty)) +
geom_point() +
geom_point(aes(y=pred), col="red") +
geom_segment(aesaes(x=seq_along(cty),y=cty,xend=seq_along(cty),yend=pred)) +
theme_minimal()
df %>%
add_predictions(m3) %>%
ggplot(aes(x=displ,y=cty)) +
geom_point() +
geom_point(aes(y=pred), col="red") +
geom_segment(aes(x=seq_along(cty),y=cty,xend=seq_along(cty),yend=pred)) +
theme_minimal()
df %>%
add_predictions(m3) %>%
ggplot(aes(x=displ,y=cty)) +
geom_point() +
geom_point(aes(y=pred), col="red") +
geom_segment(aes(x=seq_along(displ),y=cty,xend=seq_along(cty),yend=pred)) +
theme_minimal()
df %>%
add_predictions(m3) %>%
ggplot(aes(x=displ,y=cty)) +
geom_point() +
geom_point(aes(y=pred), col="red") +
geom_segment(aes(x=seq_along(displ),y=displ,xend=seq_along(cty),yend=pred)) +
theme_minimal()
df %>%
add_predictions(m3) %>%
ggplot(aes(x=displ,y=cty)) +
geom_point() +
geom_point(aes(y=pred), col="red") +
geom_segment(aes(x=seq_along(displ),y=displ,xend=seq_along(displ),yend=pred)) +
theme_minimal()
df %>%
add_predictions(m3) %>%
ggplot(aes(x=displ,y=cty)) +
geom_point() +
geom_point(aes(y=pred), col="red") +
geom_segment(aes(x=seq_along(displ),y=displ,xend=seq_along(displ),yend=cty)) +
theme_minimal()
df %>%
add_predictions(m3) %>%
ggplot(aes(x=displ,y=cty)) +
geom_point() +
geom_point(aes(y=pred), col="red") +
geom_segment(aes(x=seq_along(displ),y=cty,xend=seq_along(displ),yend=cty)) +
theme_minimal()
df %>%
add_predictions(m3) %>%
ggplot(aes(x=displ,y=cty)) +
geom_point() +
geom_point(aes(y=pred), col="red") +
geom_segment(aes(x=seq_along(displ),y=pred,xend=seq_along(displ),yend=cty)) +
theme_minimal()
df %>%
add_predictions(m3) %>%
ggplot(aes(x=displ,y=cty)) +
geom_point() +
geom_point(aes(y=pred), col="red") +
geom_segment(aes(x=seq_along(pred),y=pred,xend=seq_along(displ),yend=cty)) +
theme_minimal()
df %>%
add_predictions(m3) %>%
ggplot(aes(x=displ,y=cty)) +
geom_point() +
geom_point(aes(y=pred), col="red") +
geom_segment(aes(x=seq_along(pred),y=pred,xend=seq_along(pred),yend=cty)) +
theme_minimal()
df %>%
add_predictions(m3) %>%
ggplot(aes(x=displ,y=cty)) +
geom_point() +
geom_point(aes(y=pred), col="red") +
geom_segment(aes(x=seq_along(pred),y=pred,xend=seq_along(pred),yend=displ)) +
theme_minimal()
df %>%
add_predictions(m3) %>%
ggplot(aes(x=displ,y=cty)) +
geom_point() +
geom_point(aes(y=pred), col="red") +
geom_segment(aes(x=seq_along(displ),y=cty,xend=seq_along(displ),yend=pred)) +
theme_minimal()
# lme4 package
# lmer test package - gives p-value
# stats class do frequentist stats
# the other type is bayesian stats - will spit out a distribution
#     can do tidymodels to do bayesian to look smart
library(tidyverse)
data.frame(x = rnorm(100)) %>%
ggplot(aes(x)) +
geom_density()
rnorm(10)
data.frame(x = rnorm(10)) %>%
ggplot(aes(x)) +
geom_density()
data.frame(x = rnorm(10)) %>%
ggplot(aes(x)) +
geom_density()
data.frame(x = rnorm(100)) %>%
ggplot(aes(x)) +
geom_density()
data.frame(x = rnorm(1000)) %>%
ggplot(aes(x)) +
geom_density()
data.frame(x = rnorm(10000)) %>%
ggplot(aes(x)) +
geom_density()
data.frame(x = rnorm(100000)) %>%
ggplot(aes(x)) +
geom_density()
data.frame(x = rnorm(1000000)) %>%
ggplot(aes(x)) +
geom_density()
data.frame(x = rnorm(1000000, mean = 5)) %>%
ggplot(aes(x)) +
geom_density()
data.frame(x = rnorm(1000000, mean = 5, se = 10)) %>%
ggplot(aes(x)) +
geom_density()
data.frame(x = rnorm(1000000, mean = 5, sd = 10)) %>%
ggplot(aes(x)) +
geom_density()
rbinom(10,10,0.5)
rbinom(10,1,0.5)
rbinom(10,1,0.5)
rbinom(10,1,0.5)
rbinom(10,10,0.5)
rbinom(50,10,0.5)
rbinom(1,100,0.5)
rbinom(1,100,0.5)
rbinom(1,100,0.5)
rbinom(1,100,0.5)
rbinom(1,100,0.5)
rbinom(1,100,0.5)
rbinom(1,100,0.5)
rbinom(1,100,0.5)
rbinom(1,100,0.5)
rbinom(1,100,0.5)
rbinom(1,100,0.5)
rbinom(1,100,0.5)
rbinom(1,100,0.5)
for(i in 1:1000){
print(rbinom(1,100,0.5))
}
x <- c()
for(i in 1:1000){
x[i] <- (rbinom(1,100,0.5))
}
data.frame(x) %>%
ggplot(aes(x)) +
geom_density()
